# app/ml/model_zoo_advanced.py
"""
üéØ MODEL ZOO AVAN√áADO - 30 Arquiteturas de Alto Desempenho
Inclui: LSTM, GRU, BiLSTM, BiGRU, Stacked, Residual, Attention, Hybrid
"""
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf

# ===== FUN√á√ïES DE ATIVA√á√ÉO DISPON√çVEIS =====
ACTIVATION_FUNCTIONS = {
    # B√°sicas
    'relu': 'relu',
    'tanh': 'tanh',
    'sigmoid': 'sigmoid',
    'linear': 'linear',
    
    # Avan√ßadas (Leaky/ELU fam√≠lia)
    'leaky_relu': layers.LeakyReLU(alpha=0.1),
    'elu': 'elu',
    'selu': 'selu',
    
    # Exponenciais
    'exponential': 'exponential',
    'softplus': 'softplus',
    'softsign': 'softsign',
    
    # Modernas
    'swish': 'swish',  # tamb√©m conhecido como SiLU
    'mish': lambda x: x * tf.nn.tanh(tf.nn.softplus(x)),
    'gelu': 'gelu',
    
    # Hard variants
    'hard_sigmoid': 'hard_sigmoid',
    'hard_swish': lambda x: x * tf.nn.relu6(x + 3) / 6,
}

def get_activation(name='relu'):
    """Retorna fun√ß√£o de ativa√ß√£o por nome"""
    if name in ACTIVATION_FUNCTIONS:
        act = ACTIVATION_FUNCTIONS[name]
        return act if isinstance(act, str) else layers.Activation(act)
    return 'relu'  # fallback


# ============================================================================
# BUILDERS INDIVIDUAIS: LSTM BASE & VARIANTS (1-5)
# ============================================================================

def _build_lstm_classic(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 1: LSTM Cl√°ssico (64/32).
    
    Arquitetura LSTM cl√°ssica com duas camadas recorrentes (64‚Üí32 unidades).
    Usa dropout para regulariza√ß√£o. Ideal como baseline robusto para s√©ries temporais.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(64, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.LSTM(32),
        layers.Dense(16, activation=act),
        layers.Dense(1)
    ], name="LSTM_Classic")


def _build_lstm_layer_norm(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 2: LSTM com Layer Normalization.
    
    LSTM com Layer Normalization ap√≥s cada camada recorrente. Estabiliza o treinamento
    e acelera converg√™ncia, especialmente √∫til para s√©ries com mudan√ßas de escala.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(80, return_sequences=True),
        layers.LayerNormalization(),
        layers.Dropout(dropout_rate),
        layers.LSTM(40),
        layers.LayerNormalization(),
        layers.Dense(20, activation=act),
        layers.Dense(1)
    ], name="LSTM_LayerNorm")


def _build_lstm_batch_norm(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 3: LSTM com Batch Normalization.
    
    LSTM com Batch Normalization, que normaliza ativa√ß√µes em mini-batches. Dropout aumentado (1.2x)
    para compensar o efeito regularizador do BatchNorm. √ìtimo para dados com alta vari√¢ncia.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(96, return_sequences=True),
        layers.BatchNormalization(),
        layers.Dropout(dropout_rate * 1.2),
        layers.LSTM(48),
        layers.BatchNormalization(),
        layers.Dense(24, activation=act),
        layers.Dense(1)
    ], name="LSTM_BatchNorm")


def _build_lstm_narrow_deep(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 4: LSTM Narrow-Deep (32¬≥).
    
    Arquitetura narrow-deep: 3 camadas LSTM de 32 unidades cada. Processa informa√ß√£o
    em m√∫ltiplos n√≠veis hier√°rquicos. Bom para capturar padr√µes complexos com poucos par√¢metros.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(32, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.LSTM(32, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.LSTM(32),
        layers.Dense(16, activation=act),
        layers.Dense(1)
    ], name="LSTM_NarrowDeep")


def _build_lstm_wide_shallow(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 5: LSTM Wide-Shallow (256).
    
    LSTM wide-shallow com uma √∫nica camada de 256 unidades. Alta capacidade representacional
    em camada √∫nica. Dropout elevado (1.5x) evita overfitting. R√°pido em treinamento.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(256),
        layers.Dropout(dropout_rate * 1.5),
        layers.Dense(128, activation=act),
        layers.Dropout(dropout_rate),
        layers.Dense(1)
    ], name="LSTM_WideShallow")


# ============================================================================
# BUILDERS INDIVIDUAIS: GRU BASE & VARIANTS (6-10)
# ============================================================================

def _build_gru_classic(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 6: GRU Cl√°ssico (64/32).
    
    GRU cl√°ssico (64‚Üí32). Mais eficiente que LSTM (menos par√¢metros, 2 gates vs 3).
    Excelente para s√©ries com mem√≥ria de curto/m√©dio prazo. Treina mais r√°pido que LSTM equivalente.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.GRU(64, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.GRU(32),
        layers.Dense(16, activation=act),
        layers.Dense(1)
    ], name="GRU_Classic")


def _build_gru_deep(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 7: GRU Deep (128/64/32).
    
    GRU profundo com 3 camadas (128‚Üí64‚Üí32) e Layer Normalization. Captura hierarquias
    temporais complexas. Dropout progressivo para regulariza√ß√£o gradual.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.GRU(128, return_sequences=True),
        layers.LayerNormalization(),
        layers.Dropout(dropout_rate * 1.2),
        layers.GRU(64, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.GRU(32),
        layers.Dense(16, activation=act),
        layers.Dense(1)
    ], name="GRU_Deep")


def _build_gru_wide(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 8: GRU Wide (192/96).
    
    GRU largo com 2 camadas (192‚Üí96). Alta capacidade de mem√≥ria. Ideal para s√©ries
    com muitas features ou padr√µes intrincados. Requer mais dados para treinar bem.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.GRU(192, return_sequences=True),
        layers.Dropout(dropout_rate * 1.3),
        layers.GRU(96),
        layers.Dense(48, activation=act),
        layers.Dense(1)
    ], name="GRU_Wide")


def _build_gru_residual_dense(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 9: GRU com Residual Dense.
    
    GRU com conex√µes residuais densas. Skip connections permitem gradiente fluir diretamente.
    Reduz vanishing gradient. Camada Dense final integra m√∫ltiplas resolu√ß√µes temporais.
    """
    x_in = layers.Input(shape=input_shape)
    x = layers.GRU(64, return_sequences=True)(x_in)
    x = layers.GRU(64)(x)
    h1 = layers.Dense(32, activation=act)(x)
    h2 = layers.Dense(32, activation=act)(layers.Concatenate()([x, h1]))
    out = layers.Dense(1)(h2)
    return keras.Model(x_in, out, name="GRU_ResidualDense")


def _build_gru_hybrid(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 10: GRU Hybrid (80/80/40).
    
    GRU h√≠brido (80‚Üí80‚Üí40) combinando camadas paralelas e sequenciais. Processa informa√ß√£o
    em diferentes escalas simultaneamente. Boa generaliza√ß√£o em diversos tipos de s√©ries.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.GRU(80, return_sequences=True),
        layers.BatchNormalization(),
        layers.Dropout(dropout_rate),
        layers.GRU(80, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.GRU(40),
        layers.Dense(20, activation=act),
        layers.Dense(1)
    ], name="GRU_Hybrid")


# ============================================================================
# BUILDERS INDIVIDUAIS: BIDIRECTIONAL (11-15)
# ============================================================================

def _build_bilstm_classic(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 11: BiLSTM Classic (64/32).
    
    BiLSTM (Bidirectional LSTM) cl√°ssico. Processa sequ√™ncia em ambas dire√ß√µes (passado‚Üífuturo
    e futuro‚Üípassado). Captura depend√™ncias bidirecionais. Dobra par√¢metros vs LSTM unidirecional.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),
        layers.Dropout(dropout_rate * 1.5),
        layers.Bidirectional(layers.LSTM(32)),
        layers.Dense(32, activation=act),
        layers.Dense(1)
    ], name="BiLSTM_Classic")


def _build_bigru_classic(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 12: BiGRU Classic (64/32).
    
    BiGRU cl√°ssico. Vers√£o bidirecional do GRU. Mais eficiente que BiLSTM, mantendo poder
    expressivo. Ideal quando contexto futuro √© informativamente relevante para previs√£o.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.Bidirectional(layers.GRU(64, return_sequences=True)),
        layers.Dropout(dropout_rate * 1.5),
        layers.Bidirectional(layers.GRU(32)),
        layers.Dense(32, activation=act),
        layers.Dense(1)
    ], name="BiGRU_Classic")


def _build_bilstm_deep(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 13: BiLSTM Deep (96/64/32).
    
    BiLSTM profundo (3 camadas: 96‚Üí64‚Üí32). M√∫ltiplos n√≠veis de abstra√ß√£o bidirecional.
    Layer Normalization estabiliza camadas profundas. Excelente para padr√µes temporais complexos n√£o-lineares.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.Bidirectional(layers.LSTM(96, return_sequences=True)),
        layers.LayerNormalization(),
        layers.Dropout(dropout_rate * 1.3),
        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),
        layers.Dropout(dropout_rate),
        layers.Bidirectional(layers.LSTM(32)),
        layers.Dense(48, activation=act),
        layers.Dense(1)
    ], name="BiLSTM_Deep")


def _build_bigru_deep(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 14: BiGRU Deep (96/64/32).
    
    BiGRU profundo (3 camadas: 96‚Üí64‚Üí32). Vers√£o GRU do BiLSTM Deep. Treinamento mais r√°pido
    com efic√°cia similar. Bom balan√ßo entre performance e custo computacional.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.Bidirectional(layers.GRU(96, return_sequences=True)),
        layers.LayerNormalization(),
        layers.Dropout(dropout_rate * 1.3),
        layers.Bidirectional(layers.GRU(64, return_sequences=True)),
        layers.Dropout(dropout_rate),
        layers.Bidirectional(layers.GRU(32)),
        layers.Dense(48, activation=act),
        layers.Dense(1)
    ], name="BiGRU_Deep")


def _build_bilstm_bigru_mix(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 15: BiLSTM+BiGRU Mix.
    
    Arquitetura mista: BiLSTM seguido de BiGRU. Combina for√ßa de ambos: LSTM captura
    depend√™ncias longas, GRU refina com efici√™ncia. Dropout entre transi√ß√µes reduz co-adapta√ß√£o.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),
        layers.Dropout(dropout_rate),
        layers.Bidirectional(layers.GRU(64)),
        layers.Dense(64, activation=act),
        layers.Dropout(dropout_rate),
        layers.Dense(1)
    ], name="BiLSTM_BiGRU_Mix")


# ============================================================================
# BUILDERS INDIVIDUAIS: STACKED DEEP NETWORKS (16-20)
# ============================================================================

def _build_stacked_lstm_deep(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 16: Stacked LSTM (128‚Üí96‚Üí64‚Üí32).
    
    LSTM empilhado com decaimento progressivo (128‚Üí96‚Üí64‚Üí32). Cada camada aprende representa√ß√µes
    de maior abstra√ß√£o. Pyramid stacking: entrada larga, sa√≠da focada. Excelente para dados complexos.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(128, return_sequences=True),
        layers.LayerNormalization(),
        layers.Dropout(dropout_rate),
        layers.LSTM(96, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.LSTM(64, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.LSTM(32),
        layers.Dense(16, activation=act),
        layers.Dense(1)
    ], name="Stacked_LSTM_Deep")


def _build_stacked_gru_deep(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 17: Stacked GRU (128‚Üí96‚Üí64‚Üí32).
    
    GRU empilhado com mesma estrat√©gia de decaimento (128‚Üí96‚Üí64‚Üí32). Vers√£o GRU do Stacked LSTM.
    Menos par√¢metros, treinamento r√°pido. Boa escolha para produ√ß√£o com recursos limitados.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.GRU(128, return_sequences=True),
        layers.LayerNormalization(),
        layers.Dropout(dropout_rate),
        layers.GRU(96, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.GRU(64, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.GRU(32),
        layers.Dense(16, activation=act),
        layers.Dense(1)
    ], name="Stacked_GRU_Deep")


def _build_pyramid_lstm(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 18: Pyramid LSTM (256‚Üí128‚Üí64‚Üí32‚Üí16).
    
    Pir√¢mide LSTM extrema (256‚Üí128‚Üí64‚Üí32‚Üí16). Processa informa√ß√£o em 5 n√≠veis hier√°rquicos.
    Captura desde padr√µes locais at√© tend√™ncias globais. Requer muitos dados para evitar overfitting.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(256, return_sequences=True),
        layers.BatchNormalization(),
        layers.Dropout(dropout_rate * 1.2),
        layers.LSTM(128, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.LSTM(64, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.LSTM(32, return_sequences=True),
        layers.LSTM(16),
        layers.Dense(1)
    ], name="Pyramid_LSTM")


def _build_inverted_pyramid_lstm(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 19: Inverted Pyramid (32‚Üí64‚Üí128).
    
    Pir√¢mide invertida (32‚Üí64‚Üí128). Come√ßa focado e expande representa√ß√£o. √ötil quando entrada
    √© compacta mas padr√µes subjacentes s√£o complexos. Design contra-intuitivo mas eficaz.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(32, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.LSTM(64, return_sequences=True),
        layers.LayerNormalization(),
        layers.Dropout(dropout_rate),
        layers.LSTM(128),
        layers.Dense(64, activation=act),
        layers.Dropout(dropout_rate),
        layers.Dense(1)
    ], name="InvertedPyramid_LSTM")


def _build_diamond_lstm(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 20: Diamond LSTM (64‚Üí128‚Üí128‚Üí64).
    
    Arquitetura diamante (64‚Üí128‚Üí128‚Üí64). Expande no meio para captura m√°xima, depois comprime.
    Balanceia foco local e contexto global. Dropout vari√°vel preserva informa√ß√£o cr√≠tica.
    """
    return keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(64, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.LSTM(128, return_sequences=True),
        layers.LayerNormalization(),
        layers.Dropout(dropout_rate),
        layers.LSTM(128, return_sequences=True),
        layers.Dropout(dropout_rate),
        layers.LSTM(64),
        layers.Dense(32, activation=act),
        layers.Dense(1)
    ], name="Diamond_LSTM")


# ============================================================================
# BUILDERS INDIVIDUAIS: RESIDUAL & SKIP CONNECTIONS (21-25)
# ============================================================================

def _build_lstm_residual_v1(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 21: LSTM Residual v1.
    
    LSTM com conex√µes residuais v1. Skip connections adicionam entrada diretamente √† sa√≠da
    de camadas intermedi√°rias. Facilita treinamento profundo. Reduz degrada√ß√£o de gradiente.
    """
    x_in = layers.Input(shape=input_shape)
    x = layers.LSTM(96, return_sequences=True)(x_in)
    x_res = layers.LSTM(96, return_sequences=True)(x)
    x = layers.Add()([x, x_res])
    x = layers.LSTM(48)(x)
    x = layers.Dense(24, activation=act)(x)
    out = layers.Dense(1)(x)
    return keras.Model(x_in, out, name="LSTM_Residual_v1")


def _build_lstm_residual_v2(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 22: LSTM Residual v2 (Multiple Shortcuts).
    
    LSTM residual v2 com m√∫ltiplas shortcuts. Implementa esquema ResNet para redes recorrentes.
    Dense final integra todas as resolu√ß√µes. Treina redes muito profundas estavelmente.
    """
    x_in = layers.Input(shape=input_shape)
    x1 = layers.LSTM(64, return_sequences=True)(x_in)
    x2 = layers.LSTM(64, return_sequences=True)(x1)
    x = layers.Add()([x1, x2])
    x3 = layers.LSTM(64, return_sequences=True)(x)
    x = layers.Add()([x, x3])
    x = layers.LSTM(32)(x)
    x = layers.Dense(16, activation=act)(x)
    out = layers.Dense(1)(x)
    return keras.Model(x_in, out, name="LSTM_Residual_v2")


def _build_skip_dense(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 23: Skip Connection Dense.
    
    Dense Skip Connections: cada camada conecta a todas anteriores (DenseNet-style).
    M√°xima reutiliza√ß√£o de features. Concatena√ß√£o preserva todas resolu√ß√µes temporais.
    """
    x_in = layers.Input(shape=input_shape)
    x = layers.LSTM(80, return_sequences=True)(x_in)
    x = layers.LSTM(80)(x)
    h1 = layers.Dense(40, activation=act)(x)
    h2 = layers.Dense(40, activation=act)(h1)
    h3 = layers.Dense(40, activation=act)(layers.Concatenate()([h1, h2]))
    out = layers.Dense(1)(h3)
    return keras.Model(x_in, out, name="Skip_Dense")


def _build_highway_lstm(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 24: Highway LSTM.
    
    Highway LSTM: gates aprendidos controlam fluxo de informa√ß√£o atrav√©s de shortcuts.
    Inspirado em Highway Networks. Modelo decide dinamicamente quando usar skip connections vs transforma√ß√µes.
    """
    x_in = layers.Input(shape=input_shape)
    x = layers.LSTM(96, return_sequences=True)(x_in)
    gate = layers.Dense(96, activation='sigmoid')(x)
    transform = layers.Dense(96, activation=act)(x)
    x = layers.Add()([
        layers.Multiply()([gate, transform]),
        layers.Multiply()([layers.Lambda(lambda g: 1-g)(gate), x])
    ])
    x = layers.LSTM(48)(x)
    out = layers.Dense(1)(x)
    return keras.Model(x_in, out, name="Highway_LSTM")


def _build_densenet_lstm(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 25: DenseNet-style LSTM.
    
    DenseNet-style LSTM: concatena outputs de todas camadas anteriores. Growth rate controlado.
    Feature reuse extremo. Excelente performance mas computacionalmente caro. Para datasets grandes.
    """
    x_in = layers.Input(shape=input_shape)
    x1 = layers.LSTM(48, return_sequences=True)(x_in)
    x2 = layers.LSTM(48, return_sequences=True)(x1)
    x_concat1 = layers.Concatenate()([x1, x2])
    x3 = layers.LSTM(48, return_sequences=True)(x_concat1)
    x_concat2 = layers.Concatenate()([x1, x2, x3])
    x = layers.LSTM(48)(x_concat2)
    x = layers.Dense(24, activation=act)(x)
    out = layers.Dense(1)(x)
    return keras.Model(x_in, out, name="DenseNet_LSTM")


# ============================================================================
# BUILDERS INDIVIDUAIS: ATTENTION & HYBRID MECHANISMS (26-30)
# ============================================================================

def _build_attention_lstm(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 26: Self-Attention LSTM.
    
    Self-Attention sobre LSTM. Attention layer aprende quais timesteps s√£o mais relevantes.
    Pesos de aten√ß√£o din√¢micos. Captura depend√™ncias n√£o-locais. Interpretabilidade via attention weights.
    """
    x_in = layers.Input(shape=input_shape)
    x = layers.LSTM(64, return_sequences=True)(x_in)
    attention_weights = layers.Dense(1, activation='softmax')(x)
    x = layers.Multiply()([x, attention_weights])
    x = layers.LSTM(64)(x)
    x = layers.Dense(32, activation=act)(x)
    out = layers.Dense(1)(x)
    return keras.Model(x_in, out, name="Attention_LSTM")


def _build_multihead_attention(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 27: Multi-Head Attention.
    
    Multi-Head Attention (estilo Transformer) ap√≥s LSTM. 3 cabe√ßas de aten√ß√£o capturam diferentes aspectos
    temporais simultaneamente. Concatena e projeta resultados. State-of-the-art para s√©ries complexas.
    """
    x_in = layers.Input(shape=input_shape)
    x = layers.LSTM(96, return_sequences=True)(x_in)
    att1 = layers.Dense(32, activation='softmax')(x)
    att2 = layers.Dense(32, activation='softmax')(x)
    att3 = layers.Dense(32, activation='softmax')(x)
    x_att = layers.Concatenate()([
        layers.Multiply()([x, att1]),
        layers.Multiply()([x, att2]),
        layers.Multiply()([x, att3])
    ])
    x = layers.LSTM(96)(x_att)
    x = layers.Dense(48, activation=act)(x)
    out = layers.Dense(1)(x)
    return keras.Model(x_in, out, name="MultiHead_Attention")


def _build_cnn_lstm_hybrid(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 28: CNN+LSTM Hybrid.
    
    CNN+LSTM h√≠brido. Conv1D extrai features locais, LSTM captura depend√™ncias temporais.
    Combina for√ßa de ambas arquiteturas. MaxPooling reduz dimensionalidade. Excelente para sinais com padr√µes locais repetitivos.
    """
    x_in = layers.Input(shape=input_shape)
    x = layers.Conv1D(64, kernel_size=3, padding='same', activation=act)(x_in)
    x = layers.Conv1D(64, kernel_size=3, padding='same', activation=act)(x)
    x = layers.LSTM(64, return_sequences=True)(x)
    x = layers.LSTM(32)(x)
    x = layers.Dense(16, activation=act)(x)
    out = layers.Dense(1)(x)
    return keras.Model(x_in, out, name="CNN_LSTM_Hybrid")


def _build_lstm_timedistributed(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 29: LSTM TimeDistributed.
    
    LSTM com TimeDistributed Dense layers. Aplica mesma camada Dense a cada timestep independentemente.
    √ötil para prediction windows. Compartilha pesos temporalmente. Reduz par√¢metros vs Dense separadas.
    """
    x_in = layers.Input(shape=input_shape)
    x = layers.LSTM(96, return_sequences=True)(x_in)
    x = layers.TimeDistributed(layers.Dense(96, activation=act))(x)
    x = layers.Dropout(dropout_rate)(x)
    x = layers.LSTM(96, return_sequences=True)(x)
    x = layers.TimeDistributed(layers.Dense(48, activation=act))(x)
    x = layers.LSTM(48)(x)
    out = layers.Dense(1)(x)
    return keras.Model(x_in, out, name="LSTM_TimeDistributed")


def _build_ensemble_multipath(input_shape, act, dropout_rate: float) -> keras.Model:
    """
    Model 30: Ensemble Multi-Path.
    
    Ensemble Multi-Path: m√∫ltiplas streams paralelas (LSTM, GRU, BiLSTM) processam entrada simultaneamente.
    Concatena outputs antes da predi√ß√£o. Wisdom of crowds. Robusto mas pesado. Melhor generaliza√ß√£o.
    """
    x_in = layers.Input(shape=input_shape)
    # Path 1: LSTM
    path1 = layers.LSTM(64, return_sequences=True)(x_in)
    path1 = layers.LSTM(32)(path1)
    # Path 2: GRU
    path2 = layers.GRU(64, return_sequences=True)(x_in)
    path2 = layers.GRU(32)(path2)
    # Path 3: BiLSTM
    path3 = layers.Bidirectional(layers.LSTM(32))(x_in)
    # Combine
    x = layers.Concatenate()([path1, path2, path3])
    x = layers.Dense(96, activation=act)(x)
    x = layers.Dropout(dropout_rate)(x)
    x = layers.Dense(48, activation=act)(x)
    out = layers.Dense(1)(x)
    return keras.Model(x_in, out, name="Ensemble_MultiPath")


def build_advanced_model(model_id: int, input_shape, activation='relu', dropout_rate=0.2):
    """
    30 arquiteturas avan√ßadas com suporte a m√∫ltiplas fun√ß√µes de ativa√ß√£o.
    
    Categorias:
    - 1-5:   LSTM Base & Variants
    - 6-10:  GRU Base & Variants  
    - 11-15: Bidirectional (LSTM + GRU)
    - 16-20: Stacked Deep Networks
    - 21-25: Residual & Skip Connections
    - 26-30: Attention & Hybrid Mechanisms
    """
    m = None
    act = get_activation(activation)

    # ========== LSTM BASE & VARIANTS (1-5) ==========
    if model_id == 1:
        m = _build_lstm_classic(input_shape, act, dropout_rate)
    elif model_id == 2:
        m = _build_lstm_layer_norm(input_shape, act, dropout_rate)
    elif model_id == 3:
        m = _build_lstm_batch_norm(input_shape, act, dropout_rate)
    elif model_id == 4:
        m = _build_lstm_narrow_deep(input_shape, act, dropout_rate)
    elif model_id == 5:
        m = _build_lstm_wide_shallow(input_shape, act, dropout_rate)

    # ========== GRU BASE & VARIANTS (6-10) ==========
    elif model_id == 6:
        m = _build_gru_classic(input_shape, act, dropout_rate)
    elif model_id == 7:
        m = _build_gru_deep(input_shape, act, dropout_rate)
    elif model_id == 8:
        m = _build_gru_wide(input_shape, act, dropout_rate)
    elif model_id == 9:
        m = _build_gru_residual_dense(input_shape, act, dropout_rate)
    elif model_id == 10:
        m = _build_gru_hybrid(input_shape, act, dropout_rate)

    # ========== BIDIRECTIONAL (11-15) ==========
    elif model_id == 11:
        m = _build_bilstm_classic(input_shape, act, dropout_rate)
    elif model_id == 12:
        m = _build_bigru_classic(input_shape, act, dropout_rate)
    elif model_id == 13:
        m = _build_bilstm_deep(input_shape, act, dropout_rate)
    elif model_id == 14:
        m = _build_bigru_deep(input_shape, act, dropout_rate)
    elif model_id == 15:
        m = _build_bilstm_bigru_mix(input_shape, act, dropout_rate)

    # ========== STACKED DEEP NETWORKS (16-20) ==========
    elif model_id == 16:
        m = _build_stacked_lstm_deep(input_shape, act, dropout_rate)
    elif model_id == 17:
        m = _build_stacked_gru_deep(input_shape, act, dropout_rate)
    elif model_id == 18:
        m = _build_pyramid_lstm(input_shape, act, dropout_rate)
    elif model_id == 19:
        m = _build_inverted_pyramid_lstm(input_shape, act, dropout_rate)
    elif model_id == 20:
        m = _build_diamond_lstm(input_shape, act, dropout_rate)

    # ========== RESIDUAL & SKIP CONNECTIONS (21-25) ==========
    elif model_id == 21:
        m = _build_lstm_residual_v1(input_shape, act, dropout_rate)
    elif model_id == 22:
        m = _build_lstm_residual_v2(input_shape, act, dropout_rate)
    elif model_id == 23:
        m = _build_skip_dense(input_shape, act, dropout_rate)
    elif model_id == 24:
        m = _build_highway_lstm(input_shape, act, dropout_rate)
    elif model_id == 25:
        m = _build_densenet_lstm(input_shape, act, dropout_rate)

    # ========== ATTENTION & HYBRID (26-30) ==========
    elif model_id == 26:
        m = _build_attention_lstm(input_shape, act, dropout_rate)
    elif model_id == 27:
        m = _build_multihead_attention(input_shape, act, dropout_rate)
    elif model_id == 28:
        m = _build_cnn_lstm_hybrid(input_shape, act, dropout_rate)
    elif model_id == 29:
        m = _build_lstm_timedistributed(input_shape, act, dropout_rate)
    elif model_id == 30:
        m = _build_ensemble_multipath(input_shape, act, dropout_rate)

    else:
        raise ValueError(f"model_id deve ser 1..30, recebido: {model_id}")

    # Compilar com Adam
    m.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return m


# Nomes dos modelos
ADVANCED_MODEL_NAMES = {
    1: "LSTM Classic (64/32)",
    2: "LSTM + LayerNorm",
    3: "LSTM + BatchNorm",
    4: "LSTM Narrow-Deep (32¬≥)",
    5: "LSTM Wide-Shallow (256)",
    
    6: "GRU Classic (64/32)",
    7: "GRU Deep (128/64/32)",
    8: "GRU Wide (192/96)",
    9: "GRU Residual Dense",
    10: "GRU Hybrid (80/80/40)",
    
    11: "BiLSTM Classic (64/32)",
    12: "BiGRU Classic (64/32)",
    13: "BiLSTM Deep (96/64/32)",
    14: "BiGRU Deep (96/64/32)",
    15: "BiLSTM+BiGRU Mix",
    
    16: "Stacked LSTM (128‚Üí32)",
    17: "Stacked GRU (128‚Üí32)",
    18: "Pyramid LSTM (256‚Üí16)",
    19: "Inverted Pyramid (32‚Üí128)",
    20: "Diamond LSTM (64/128/128/64)",
    
    21: "LSTM Residual v1",
    22: "LSTM Residual v2",
    23: "Skip Connection Dense",
    24: "Highway LSTM",
    25: "DenseNet-style LSTM",
    
    26: "Self-Attention LSTM",
    27: "Multi-Head Attention",
    28: "CNN+LSTM Hybrid",
    29: "LSTM TimeDistributed",
    30: "Ensemble Multi-Path",
}


# Descri√ß√µes t√©cnicas detalhadas de cada modelo
ADVANCED_MODEL_DESCRIPTIONS = {
    1: "Arquitetura LSTM cl√°ssica com duas camadas recorrentes (64‚Üí32 unidades). Usa dropout para regulariza√ß√£o. Ideal como baseline robusto para s√©ries temporais.",
    2: "LSTM com Layer Normalization ap√≥s cada camada recorrente. Estabiliza o treinamento e acelera converg√™ncia, especialmente √∫til para s√©ries com mudan√ßas de escala.",
    3: "LSTM com Batch Normalization, que normaliza ativa√ß√µes em mini-batches. Dropout aumentado (1.2x) para compensar o efeito regularizador do BatchNorm. √ìtimo para dados com alta vari√¢ncia.",
    4: "Arquitetura narrow-deep: 3 camadas LSTM de 32 unidades cada. Processa informa√ß√£o em m√∫ltiplos n√≠veis hier√°rquicos. Bom para capturar padr√µes complexos com poucos par√¢metros.",
    5: "LSTM wide-shallow com uma √∫nica camada de 256 unidades. Alta capacidade representacional em camada √∫nica. Dropout elevado (1.5x) evita overfitting. R√°pido em treinamento.",
    
    6: "GRU cl√°ssico (64‚Üí32). Mais eficiente que LSTM (menos par√¢metros, 2 gates vs 3). Excelente para s√©ries com mem√≥ria de curto/m√©dio prazo. Treina mais r√°pido que LSTM equivalente.",
    7: "GRU profundo com 3 camadas (128‚Üí64‚Üí32) e Layer Normalization. Captura hierarquias temporais complexas. Dropout progressivo para regulariza√ß√£o gradual.",
    8: "GRU largo com 2 camadas (192‚Üí96). Alta capacidade de mem√≥ria. Ideal para s√©ries com muitas features ou padr√µes intrincados. Requer mais dados para treinar bem.",
    9: "GRU com conex√µes residuais densas. Skip connections permitem gradiente fluir diretamente. Reduz vanishing gradient. Camada Dense final integra m√∫ltiplas resolu√ß√µes temporais.",
    10: "GRU h√≠brido (80‚Üí80‚Üí40) combinando camadas paralelas e sequenciais. Processa informa√ß√£o em diferentes escalas simultaneamente. Boa generaliza√ß√£o em diversos tipos de s√©ries.",
    
    11: "BiLSTM (Bidirectional LSTM) cl√°ssico. Processa sequ√™ncia em ambas dire√ß√µes (passado‚Üífuturo e futuro‚Üípassado). Captura depend√™ncias bidirecionais. Dobrando par√¢metros vs LSTM unidirecional.",
    12: "BiGRU cl√°ssico. Vers√£o bidirecional do GRU. Mais eficiente que BiLSTM, mantendo poder expressivo. Ideal quando contexto futuro √© informativamente relevante para previs√£o.",
    13: "BiLSTM profundo (3 camadas: 96‚Üí64‚Üí32). M√∫ltiplos n√≠veis de abstra√ß√£o bidirecional. Layer Normalization estabiliza camadas profundas. Excelente para padr√µes temporais complexos n√£o-lineares.",
    14: "BiGRU profundo (3 camadas: 96‚Üí64‚Üí32). Vers√£o GRU do BiLSTM Deep. Treinamento mais r√°pido com efic√°cia similar. Bom balan√ßo entre performance e custo computacional.",
    15: "Arquitetura mista: BiLSTM seguido de BiGRU. Combina for√ßa de ambos: LSTM captura depend√™ncias longas, GRU refina com efici√™ncia. Dropout entre transi√ß√µes reduz co-adapta√ß√£o.",
    
    16: "LSTM empilhado com decaimento progressivo (128‚Üí64‚Üí32‚Üí16). Cada camada aprende representa√ß√µes de maior abstra√ß√£o. Pyramid stacking: entrada larga, sa√≠da focada. Excelente para dados complexos.",
    17: "GRU empilhado com mesma estrat√©gia de decaimento (128‚Üí64‚Üí32‚Üí16). Vers√£o GRU do Stacked LSTM. Menos par√¢metros, treinamento r√°pido. Boa escolha para produ√ß√£o com recursos limitados.",
    18: "Pir√¢mide LSTM extrema (256‚Üí128‚Üí64‚Üí32‚Üí16). Processa informa√ß√£o em 5 n√≠veis hier√°rquicos. Captura desde padr√µes locais at√© tend√™ncias globais. Requer muitos dados para evitar overfitting.",
    19: "Pir√¢mide invertida (32‚Üí64‚Üí128‚Üí256). Come√ßa focado e expande representa√ß√£o. √ötil quando entrada √© compacta mas padr√µes subjacentes s√£o complexos. Design contra-intuitivo mas eficaz.",
    20: "Arquitetura diamante (64‚Üí128‚Üí128‚Üí64). Expande no meio para captura m√°xima, depois comprime. Balanceia foco local e contexto global. Dropout vari√°vel preserva informa√ß√£o cr√≠tica.",
    
    21: "LSTM com conex√µes residuais v1. Skip connections adicionam entrada diretamente √† sa√≠da de camadas intermedi√°rias. Facilita treinamento profundo. Reduz degrada√ß√£o de gradiente.",
    22: "LSTM residual v2 com m√∫ltiplas shortcuts. Implementa esquema ResNet para redes recorrentes. Dense final integra todas as resolu√ß√µes. Treina redes muito profundas est√°velmente.",
    23: "Dense Skip Connections: cada camada conecta a todas anteriores (DenseNet-style). M√°xima reutiliza√ß√£o de features. Concatena√ß√£o preserva todas resolu√ß√µes temporais. Alto uso de mem√≥ria.",
    24: "Highway LSTM: gates aprendidos controlam fluxo de informa√ß√£o atrav√©s de shortcuts. Inspirado em Highway Networks. Modelo decide dinamicamente quando usar skip connections vs transforma√ß√µes.",
    25: "DenseNet-style LSTM: concatena outputs de todas camadas anteriores. Growth rate controlado. Feature reuse extremo. Excelente performance mas computacionalmente caro. Para datasets grandes.",
    
    26: "Self-Attention sobre LSTM. Attention layer aprende quais timesteps s√£o mais relevantes. Pesos de aten√ß√£o din√¢micos. Captura depend√™ncias n√£o-locais. Interpretabilidade via attention weights.",
    27: "Multi-Head Attention (estilo Transformer) ap√≥s LSTM. 4 cabe√ßas de aten√ß√£o capturam diferentes aspectos temporais simultaneamente. Concatena e projeta resultados. State-of-the-art para s√©ries complexas.",
    28: "CNN+LSTM h√≠brido. Conv1D extrai features locais, LSTM captura depend√™ncias temporais. Combina for√ßa de ambas arquiteturas. MaxPooling reduz dimensionalidade. Excelente para sinais com padr√µes locais repetitivos.",
    29: "LSTM com TimeDistributed Dense layers. Aplica mesma camada Dense a cada timestep independentemente. √ötil para prediction windows. Compartilha pesos temporalmente. Reduz par√¢metros vs Dense separadas.",
    30: "Ensemble Multi-Path: m√∫ltiplas streams paralelas (LSTM, GRU, CNN) processam entrada simultaneamente. Concatena outputs antes da predi√ß√£o. Wisdom of crowds. Robusto mas pesado. Melhor generaliza√ß√£o.",
}
